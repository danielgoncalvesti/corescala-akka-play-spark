2017-07-01 15:37:04,493  INFO [main] (notebook.kernel.pfork.BetterFork$) - Remote process starting
2017-07-01 15:37:05,792  INFO [Remote-akka.actor.default-dispatcher-2] (akka.event.slf4j.Slf4jLogger) - Slf4jLogger started
2017-07-01 15:37:06,023  INFO [Remote-akka.actor.default-dispatcher-3] (Remoting) - Starting remoting
2017-07-01 15:37:06,570  INFO [Remote-akka.actor.default-dispatcher-2] (Remoting) - Remoting started; listening on addresses :[akka.tcp://Remote@127.0.0.1:46561]
2017-07-01 15:37:06,576  INFO [Remote-akka.actor.default-dispatcher-2] (Remoting) - Remoting now listens on addresses: [akka.tcp://Remote@127.0.0.1:46561]
2017-07-01 15:37:07,615  INFO [Remote-akka.actor.default-dispatcher-4] (notebook.client.ReplCalculator) - ReplCalculator preStart
2017-07-01 15:37:07,624  INFO [Remote-akka.actor.default-dispatcher-5] (notebook.client.ReplCalculator) -  INIT SCRIPT: dummy
2017-07-01 15:37:08,757  WARN [Remote-akka.actor.default-dispatcher-2] (org.apache.hadoop.util.NativeCodeLoader) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-07-01 15:37:09,176  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.SecurityManager) - Changing view acls to: turmasabado
2017-07-01 15:37:09,181  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.SecurityManager) - Changing modify acls to: turmasabado
2017-07-01 15:37:09,185  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.SecurityManager) - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(turmasabado); users with modify permissions: Set(turmasabado)
2017-07-01 15:37:09,744  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.HttpServer) - Starting HTTP Server
2017-07-01 15:37:09,895  INFO [Remote-akka.actor.default-dispatcher-2] (org.spark-project.jetty.server.Server) - jetty-8.y.z-SNAPSHOT
2017-07-01 15:37:09,942  INFO [Remote-akka.actor.default-dispatcher-2] (org.spark-project.jetty.server.AbstractConnector) - Started SocketConnector@0.0.0.0:45962
2017-07-01 15:37:09,946  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.util.Utils) - Successfully started service 'HTTP server' on port 45962.
2017-07-01 15:37:21,689  WARN [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.util.Utils) - Your hostname, globalcode-labs resolves to a loopback address: 127.0.1.1; using 192.168.1.114 instead (on interface wlan0)
2017-07-01 15:37:21,692  WARN [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.util.Utils) - Set SPARK_LOCAL_IP if you need to bind to another address
2017-07-01 15:37:22,429  INFO [Remote-akka.actor.default-dispatcher-5] (notebook.client.ReplCalculator) - Init script processed successfully
2017-07-01 15:37:22,430  INFO [Remote-akka.actor.default-dispatcher-5] (notebook.client.ReplCalculator) -  INIT SCRIPT: class server
2017-07-01 15:37:23,125  INFO [Remote-akka.actor.default-dispatcher-5] (notebook.client.ReplCalculator) - Init script processed successfully
2017-07-01 15:37:23,125  INFO [Remote-akka.actor.default-dispatcher-5] (notebook.client.ReplCalculator) -  INIT SCRIPT: deps
2017-07-01 15:37:24,024  INFO [Remote-akka.actor.default-dispatcher-5] (notebook.client.ReplCalculator) - Init script processed successfully
2017-07-01 15:37:24,025  INFO [Remote-akka.actor.default-dispatcher-5] (notebook.client.ReplCalculator) -  INIT SCRIPT: imports
2017-07-01 15:37:24,030  INFO [Remote-akka.actor.default-dispatcher-5] (notebook.client.ReplCalculator) -  INIT SCRIPT: custom conf
2017-07-01 15:37:25,663  INFO [Remote-akka.actor.default-dispatcher-5] (notebook.client.ReplCalculator) - Init script processed successfully
2017-07-01 15:37:25,663  INFO [Remote-akka.actor.default-dispatcher-5] (notebook.client.ReplCalculator) -  INIT SCRIPT: jar:file:/home/turmasabado/spark-notebook/lib/nooostab.spark-notebook-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.jar!/scripts/init.sc
2017-07-01 15:37:29,762  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.SparkContext) - Running Spark version 1.6.1
2017-07-01 15:37:29,827  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.SecurityManager) - Changing view acls to: turmasabado
2017-07-01 15:37:29,827  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.SecurityManager) - Changing modify acls to: turmasabado
2017-07-01 15:37:29,827  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.SecurityManager) - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(turmasabado); users with modify permissions: Set(turmasabado)
2017-07-01 15:37:30,351  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.util.Utils) - Successfully started service 'sparkDriver' on port 55167.
2017-07-01 15:37:30,477  INFO [sparkDriverActorSystem-akka.actor.default-dispatcher-2] (akka.event.slf4j.Slf4jLogger) - Slf4jLogger started
2017-07-01 15:37:30,485  INFO [sparkDriverActorSystem-akka.actor.default-dispatcher-4] (Remoting) - Starting remoting
2017-07-01 15:37:30,510  INFO [sparkDriverActorSystem-akka.actor.default-dispatcher-4] (Remoting) - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.1.114:46314]
2017-07-01 15:37:30,511  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.util.Utils) - Successfully started service 'sparkDriverActorSystem' on port 46314.
2017-07-01 15:37:30,537  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.SparkEnv) - Registering MapOutputTracker
2017-07-01 15:37:30,576  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.SparkEnv) - Registering BlockManagerMaster
2017-07-01 15:37:30,608  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.storage.DiskBlockManager) - Created local directory at /tmp/blockmgr-c527014d-98fb-47fa-bd41-b7d23908d704
2017-07-01 15:37:30,623  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.storage.MemoryStore) - MemoryStore started with capacity 116.4 MB
2017-07-01 15:37:30,780  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.SparkEnv) - Registering OutputCommitCoordinator
2017-07-01 15:37:31,157  INFO [Remote-akka.actor.default-dispatcher-2] (org.spark-project.jetty.server.Server) - jetty-8.y.z-SNAPSHOT
2017-07-01 15:37:31,183  INFO [Remote-akka.actor.default-dispatcher-2] (org.spark-project.jetty.server.AbstractConnector) - Started SelectChannelConnector@0.0.0.0:4040
2017-07-01 15:37:31,183  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.util.Utils) - Successfully started service 'SparkUI' on port 4040.
2017-07-01 15:37:31,192  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.ui.SparkUI) - Started SparkUI at http://192.168.1.114:4040
2017-07-01 15:37:31,272  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.HttpFileServer) - HTTP File server directory is /tmp/spark-cab7ad51-4cbd-4ed6-8dbf-d239120227f9/httpd-5663bb8e-4d02-460b-9a20-08f78fceb64a
2017-07-01 15:37:31,272  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.HttpServer) - Starting HTTP Server
2017-07-01 15:37:31,276  INFO [Remote-akka.actor.default-dispatcher-2] (org.spark-project.jetty.server.Server) - jetty-8.y.z-SNAPSHOT
2017-07-01 15:37:31,283  INFO [Remote-akka.actor.default-dispatcher-2] (org.spark-project.jetty.server.AbstractConnector) - Started SocketConnector@0.0.0.0:45563
2017-07-01 15:37:31,283  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.util.Utils) - Successfully started service 'HTTP file server' on port 45563.
2017-07-01 15:37:31,286 ERROR [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.SparkContext) - Error adding jar (java.lang.IllegalArgumentException: /home/turmasabado/spark-notebook cannot be a directory.), was the --addJars option used?
2017-07-01 15:37:31,326  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.SparkContext) - Added JAR file:/home/turmasabado/spark-notebook/lib/common.common-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.jar at http://192.168.1.114:45563/jars/common.common-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.jar with timestamp 1498934251325
2017-07-01 15:37:31,448  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.executor.Executor) - Starting executor ID driver on host localhost
2017-07-01 15:37:31,463  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.executor.Executor) - Using REPL class URI: http://192.168.1.114:45962
2017-07-01 15:37:31,520  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.util.Utils) - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39618.
2017-07-01 15:37:31,521  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.network.netty.NettyBlockTransferService) - Server created on 39618
2017-07-01 15:37:31,524  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.storage.BlockManagerMaster) - Trying to register BlockManager
2017-07-01 15:37:31,537  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerMasterEndpoint) - Registering block manager localhost:39618 with 116.4 MB RAM, BlockManagerId(driver, localhost, 39618)
2017-07-01 15:37:31,547  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.storage.BlockManagerMaster) - Registered BlockManager
2017-07-01 15:37:34,895  INFO [Remote-akka.actor.default-dispatcher-5] (notebook.client.ReplCalculator) - Init script processed successfully
2017-07-01 15:39:12,436  WARN [Remote-akka.actor.default-dispatcher-4] (org.apache.spark.util.SizeEstimator) - Failed to check whether UseCompressedOops is set; assuming yes
2017-07-01 15:39:12,638  INFO [Remote-akka.actor.default-dispatcher-4] (org.apache.spark.storage.MemoryStore) - Block broadcast_0 stored as values in memory (estimated size 115.6 KB, free 115.6 KB)
2017-07-01 15:39:12,742  INFO [Remote-akka.actor.default-dispatcher-4] (org.apache.spark.storage.MemoryStore) - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.1 KB, free 129.7 KB)
2017-07-01 15:39:12,748  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_0_piece0 in memory on localhost:39618 (size: 14.1 KB, free: 116.4 MB)
2017-07-01 15:39:12,763  INFO [Remote-akka.actor.default-dispatcher-4] (org.apache.spark.SparkContext) - Created broadcast 0 from textFile at <console>:54
2017-07-01 15:41:31,505  INFO [Remote-akka.actor.default-dispatcher-17] (org.apache.spark.storage.MemoryStore) - Block broadcast_1 stored as values in memory (estimated size 200.0 B, free 129.9 KB)
2017-07-01 15:41:31,513  INFO [Remote-akka.actor.default-dispatcher-17] (org.apache.spark.storage.MemoryStore) - Block broadcast_1_piece0 stored as bytes in memory (estimated size 133.0 B, free 130.0 KB)
2017-07-01 15:41:31,514  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_1_piece0 in memory on localhost:39618 (size: 133.0 B, free: 116.4 MB)
2017-07-01 15:41:31,518  INFO [Remote-akka.actor.default-dispatcher-17] (org.apache.spark.SparkContext) - Created broadcast 1 from broadcast at <console>:54
2017-07-01 16:41:58,671  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.hadoop.mapred.FileInputFormat) - Total input paths to process : 1
2017-07-01 16:41:58,796  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.SparkContext) - Starting job: collect at <console>:65
2017-07-01 16:41:58,832  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Registering RDD 5 (map at <console>:64)
2017-07-01 16:41:58,838  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Got job 0 (collect at <console>:65) with 2 output partitions
2017-07-01 16:41:58,840  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Final stage: ResultStage 1 (collect at <console>:65)
2017-07-01 16:41:58,843  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Parents of final stage: List(ShuffleMapStage 0)
2017-07-01 16:41:58,847  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Missing parents: List(ShuffleMapStage 0)
2017-07-01 16:41:58,860  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at <console>:64), which has no missing parents
2017-07-01 16:41:58,899  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_2 stored as values in memory (estimated size 22.1 KB, free 152.1 KB)
2017-07-01 16:41:58,905  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.2 KB, free 159.3 KB)
2017-07-01 16:41:58,907  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_2_piece0 in memory on localhost:39618 (size: 7.2 KB, free: 116.4 MB)
2017-07-01 16:41:58,910  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:41:58,922  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at <console>:64)
2017-07-01 16:41:58,935  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 0.0 with 2 tasks
2017-07-01 16:41:59,037  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:41:59,046  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:41:59,061  INFO [Executor task launch worker-1] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 0.0 (TID 1)
2017-07-01 16:41:59,061  INFO [Executor task launch worker-0] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 0.0 (TID 0)
2017-07-01 16:41:59,082  INFO [Executor task launch worker-0] (org.apache.spark.executor.Executor) - Fetching http://192.168.1.114:45563/jars/common.common-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.jar with timestamp 1498934251325
2017-07-01 16:41:59,364  INFO [Executor task launch worker-0] (org.apache.spark.util.Utils) - Fetching http://192.168.1.114:45563/jars/common.common-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.jar to /tmp/spark-cab7ad51-4cbd-4ed6-8dbf-d239120227f9/userFiles-e140cea1-5399-42a7-933c-9ccf95c03b08/fetchFileTemp2992276453640764522.tmp
2017-07-01 16:41:59,419  INFO [Executor task launch worker-0] (org.apache.spark.executor.Executor) - Adding file:/tmp/spark-cab7ad51-4cbd-4ed6-8dbf-d239120227f9/userFiles-e140cea1-5399-42a7-933c-9ccf95c03b08/common.common-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.jar to class loader
2017-07-01 16:41:59,640  INFO [Executor task launch worker-0] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:0+5200
2017-07-01 16:41:59,641  INFO [Executor task launch worker-1] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:5200+5201
2017-07-01 16:41:59,716  INFO [Executor task launch worker-1] (org.apache.hadoop.conf.Configuration.deprecation) - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2017-07-01 16:41:59,716  INFO [Executor task launch worker-1] (org.apache.hadoop.conf.Configuration.deprecation) - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2017-07-01 16:41:59,716  INFO [Executor task launch worker-1] (org.apache.hadoop.conf.Configuration.deprecation) - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2017-07-01 16:41:59,716  INFO [Executor task launch worker-1] (org.apache.hadoop.conf.Configuration.deprecation) - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2017-07-01 16:41:59,716  INFO [Executor task launch worker-1] (org.apache.hadoop.conf.Configuration.deprecation) - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2017-07-01 16:42:00,325  INFO [Executor task launch worker-0] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 0.0 (TID 0). 2274 bytes result sent to driver
2017-07-01 16:42:00,325  INFO [Executor task launch worker-1] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 0.0 (TID 1). 2274 bytes result sent to driver
2017-07-01 16:42:00,363  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 0.0 (TID 0) in 1348 ms on localhost (1/2)
2017-07-01 16:42:00,374  INFO [task-result-getter-1] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 0.0 (TID 1) in 1326 ms on localhost (2/2)
2017-07-01 16:42:00,377  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ShuffleMapStage 0 (map at <console>:64) finished in 1,395 s
2017-07-01 16:42:00,381  INFO [task-result-getter-1] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-07-01 16:42:00,387  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - looking for newly runnable stages
2017-07-01 16:42:00,388  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - running: Set()
2017-07-01 16:42:00,389  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - waiting: Set(ResultStage 1)
2017-07-01 16:42:00,397  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - failed: Set()
2017-07-01 16:42:00,402  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ResultStage 1 (ShuffledRDD[6] at reduceByKey at <console>:65), which has no missing parents
2017-07-01 16:42:00,425  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_3 stored as values in memory (estimated size 2.5 KB, free 161.9 KB)
2017-07-01 16:42:00,431  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1529.0 B, free 163.4 KB)
2017-07-01 16:42:00,437  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_3_piece0 in memory on localhost:39618 (size: 1529.0 B, free: 116.4 MB)
2017-07-01 16:42:00,439  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:42:00,445  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ResultStage 1 (ShuffledRDD[6] at reduceByKey at <console>:65)
2017-07-01 16:42:00,445  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 1.0 with 2 tasks
2017-07-01 16:42:00,472  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0,NODE_LOCAL, 1939 bytes)
2017-07-01 16:42:00,473  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1,NODE_LOCAL, 1939 bytes)
2017-07-01 16:42:00,473  INFO [Executor task launch worker-0] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 1.0 (TID 3)
2017-07-01 16:42:00,474  INFO [Executor task launch worker-1] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 1.0 (TID 2)
2017-07-01 16:42:00,510  INFO [Executor task launch worker-0] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:42:00,513  INFO [Executor task launch worker-0] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 9 ms
2017-07-01 16:42:00,519  INFO [Executor task launch worker-1] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:42:00,521  INFO [Executor task launch worker-1] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 17 ms
2017-07-01 16:42:00,615  INFO [Executor task launch worker-0] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 1.0 (TID 3). 7074 bytes result sent to driver
2017-07-01 16:42:00,616  INFO [Executor task launch worker-1] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 1.0 (TID 2). 6094 bytes result sent to driver
2017-07-01 16:42:00,623  INFO [task-result-getter-2] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 1.0 (TID 3) in 151 ms on localhost (1/2)
2017-07-01 16:42:00,627  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 1.0 (TID 2) in 164 ms on localhost (2/2)
2017-07-01 16:42:00,627  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ResultStage 1 (collect at <console>:65) finished in 0,161 s
2017-07-01 16:42:00,631  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-07-01 16:42:00,644  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.scheduler.DAGScheduler) - Job 0 finished: collect at <console>:65, took 1,845854 s
2017-07-01 16:42:01,045  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_3_piece0 on localhost:39618 in memory (size: 1529.0 B, free: 116.4 MB)
2017-07-01 16:42:01,076  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 3
2017-07-01 16:42:01,084  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_2_piece0 on localhost:39618 in memory (size: 7.2 KB, free: 116.4 MB)
2017-07-01 16:42:01,085  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 2
2017-07-01 16:42:01,096  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned shuffle 0
2017-07-01 16:43:31,726  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.SparkContext) - Starting job: collect at <console>:65
2017-07-01 16:43:31,730  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Registering RDD 9 (map at <console>:64)
2017-07-01 16:43:31,730  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Got job 1 (collect at <console>:65) with 2 output partitions
2017-07-01 16:43:31,730  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Final stage: ResultStage 3 (collect at <console>:65)
2017-07-01 16:43:31,730  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Parents of final stage: List(ShuffleMapStage 2)
2017-07-01 16:43:31,730  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Missing parents: List(ShuffleMapStage 2)
2017-07-01 16:43:31,731  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at map at <console>:64), which has no missing parents
2017-07-01 16:43:31,734  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_4 stored as values in memory (estimated size 22.1 KB, free 152.1 KB)
2017-07-01 16:43:31,740  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.2 KB, free 159.3 KB)
2017-07-01 16:43:31,741  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_4_piece0 in memory on localhost:39618 (size: 7.2 KB, free: 116.4 MB)
2017-07-01 16:43:31,742  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:43:31,742  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at map at <console>:64)
2017-07-01 16:43:31,743  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 2.0 with 2 tasks
2017-07-01 16:43:31,746  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:43:31,746  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:43:31,747  INFO [Executor task launch worker-2] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 2.0 (TID 4)
2017-07-01 16:43:31,750  INFO [Executor task launch worker-3] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 2.0 (TID 5)
2017-07-01 16:43:31,810  INFO [Executor task launch worker-3] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:5200+5201
2017-07-01 16:43:31,818  INFO [Executor task launch worker-2] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:0+5200
2017-07-01 16:43:31,949  INFO [Executor task launch worker-3] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 2.0 (TID 5). 2274 bytes result sent to driver
2017-07-01 16:43:31,951  INFO [Executor task launch worker-2] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 2.0 (TID 4). 2274 bytes result sent to driver
2017-07-01 16:43:31,956  INFO [task-result-getter-1] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 2.0 (TID 4) in 211 ms on localhost (1/2)
2017-07-01 16:43:31,957  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 2.0 (TID 5) in 211 ms on localhost (2/2)
2017-07-01 16:43:31,957  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ShuffleMapStage 2 (map at <console>:64) finished in 0,213 s
2017-07-01 16:43:31,957  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-07-01 16:43:31,958  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - looking for newly runnable stages
2017-07-01 16:43:31,958  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - running: Set()
2017-07-01 16:43:31,958  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - waiting: Set(ResultStage 3)
2017-07-01 16:43:31,958  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - failed: Set()
2017-07-01 16:43:31,958  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ResultStage 3 (ShuffledRDD[10] at reduceByKey at <console>:65), which has no missing parents
2017-07-01 16:43:31,959  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_5 stored as values in memory (estimated size 2.5 KB, free 161.9 KB)
2017-07-01 16:43:31,964  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1514.0 B, free 163.4 KB)
2017-07-01 16:43:31,966  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_5_piece0 in memory on localhost:39618 (size: 1514.0 B, free: 116.4 MB)
2017-07-01 16:43:31,966  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:43:31,967  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ResultStage 3 (ShuffledRDD[10] at reduceByKey at <console>:65)
2017-07-01 16:43:31,967  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 3.0 with 2 tasks
2017-07-01 16:43:31,968  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 3.0 (TID 6, localhost, partition 0,NODE_LOCAL, 1939 bytes)
2017-07-01 16:43:31,969  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 3.0 (TID 7, localhost, partition 1,NODE_LOCAL, 1939 bytes)
2017-07-01 16:43:31,970  INFO [Executor task launch worker-3] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 3.0 (TID 7)
2017-07-01 16:43:31,972  INFO [Executor task launch worker-2] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 3.0 (TID 6)
2017-07-01 16:43:31,974  INFO [Executor task launch worker-2] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:43:31,978  INFO [Executor task launch worker-2] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 4 ms
2017-07-01 16:43:31,974  INFO [Executor task launch worker-3] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:43:31,984  INFO [Executor task launch worker-3] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 10 ms
2017-07-01 16:43:32,033  INFO [Executor task launch worker-3] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 3.0 (TID 7). 7074 bytes result sent to driver
2017-07-01 16:43:32,038  INFO [Executor task launch worker-2] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 3.0 (TID 6). 6094 bytes result sent to driver
2017-07-01 16:43:32,040  INFO [task-result-getter-2] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 3.0 (TID 7) in 72 ms on localhost (1/2)
2017-07-01 16:43:32,042  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 3.0 (TID 6) in 75 ms on localhost (2/2)
2017-07-01 16:43:32,042  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-07-01 16:43:32,043  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ResultStage 3 (collect at <console>:65) finished in 0,075 s
2017-07-01 16:43:32,043  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.scheduler.DAGScheduler) - Job 1 finished: collect at <console>:65, took 0,314363 s
2017-07-01 16:43:32,379  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_5_piece0 on localhost:39618 in memory (size: 1514.0 B, free: 116.4 MB)
2017-07-01 16:43:32,393  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 5
2017-07-01 16:43:32,396  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_4_piece0 on localhost:39618 in memory (size: 7.2 KB, free: 116.4 MB)
2017-07-01 16:43:32,400  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 4
2017-07-01 16:43:32,401  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned shuffle 1
2017-07-01 16:44:26,596  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.storage.MemoryStore) - Block broadcast_6 stored as values in memory (estimated size 56.7 KB, free 186.7 KB)
2017-07-01 16:44:26,794  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.storage.MemoryStore) - Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.5 KB, free 206.2 KB)
2017-07-01 16:44:26,795  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_6_piece0 in memory on localhost:39618 (size: 19.5 KB, free: 116.4 MB)
2017-07-01 16:44:26,798  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.SparkContext) - Created broadcast 6 from textFile at <console>:54
2017-07-01 16:44:33,639  INFO [Remote-akka.actor.default-dispatcher-18] (org.apache.hadoop.mapred.FileInputFormat) - Total input paths to process : 1
2017-07-01 16:44:33,687  INFO [Remote-akka.actor.default-dispatcher-18] (org.apache.spark.SparkContext) - Starting job: collect at <console>:65
2017-07-01 16:44:33,698  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Registering RDD 15 (map at <console>:64)
2017-07-01 16:44:33,698  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Got job 2 (collect at <console>:65) with 2 output partitions
2017-07-01 16:44:33,698  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Final stage: ResultStage 5 (collect at <console>:65)
2017-07-01 16:44:33,698  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Parents of final stage: List(ShuffleMapStage 4)
2017-07-01 16:44:33,698  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Missing parents: List(ShuffleMapStage 4)
2017-07-01 16:44:33,699  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ShuffleMapStage 4 (MapPartitionsRDD[15] at map at <console>:64), which has no missing parents
2017-07-01 16:44:33,701  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_7 stored as values in memory (estimated size 22.1 KB, free 228.4 KB)
2017-07-01 16:44:33,712  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.2 KB, free 235.6 KB)
2017-07-01 16:44:33,721  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_7_piece0 in memory on localhost:39618 (size: 7.2 KB, free: 116.4 MB)
2017-07-01 16:44:33,722  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:44:33,722  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[15] at map at <console>:64)
2017-07-01 16:44:33,723  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 4.0 with 2 tasks
2017-07-01 16:44:33,730  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 4.0 (TID 8, localhost, partition 0,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:44:33,731  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 4.0 (TID 9, localhost, partition 1,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:44:33,737  INFO [Executor task launch worker-4] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 4.0 (TID 8)
2017-07-01 16:44:33,752  INFO [Executor task launch worker-5] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 4.0 (TID 9)
2017-07-01 16:44:33,930  INFO [Executor task launch worker-4] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:0+5200
2017-07-01 16:44:33,933  INFO [Executor task launch worker-5] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:5200+5201
2017-07-01 16:44:34,022  INFO [Executor task launch worker-4] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 4.0 (TID 8). 2274 bytes result sent to driver
2017-07-01 16:44:34,033  INFO [task-result-getter-1] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 4.0 (TID 8) in 303 ms on localhost (1/2)
2017-07-01 16:44:34,037  INFO [Executor task launch worker-5] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 4.0 (TID 9). 2274 bytes result sent to driver
2017-07-01 16:44:34,039  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 4.0 (TID 9) in 309 ms on localhost (2/2)
2017-07-01 16:44:34,040  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-07-01 16:44:34,040  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ShuffleMapStage 4 (map at <console>:64) finished in 0,311 s
2017-07-01 16:44:34,041  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - looking for newly runnable stages
2017-07-01 16:44:34,041  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - running: Set()
2017-07-01 16:44:34,041  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - waiting: Set(ResultStage 5)
2017-07-01 16:44:34,041  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - failed: Set()
2017-07-01 16:44:34,041  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ResultStage 5 (ShuffledRDD[16] at reduceByKey at <console>:65), which has no missing parents
2017-07-01 16:44:34,044  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_8 stored as values in memory (estimated size 2.5 KB, free 238.1 KB)
2017-07-01 16:44:34,049  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_8_piece0 stored as bytes in memory (estimated size 1515.0 B, free 239.6 KB)
2017-07-01 16:44:34,051  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_8_piece0 in memory on localhost:39618 (size: 1515.0 B, free: 116.4 MB)
2017-07-01 16:44:34,057  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:44:34,058  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ResultStage 5 (ShuffledRDD[16] at reduceByKey at <console>:65)
2017-07-01 16:44:34,058  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 5.0 with 2 tasks
2017-07-01 16:44:34,060  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 5.0 (TID 10, localhost, partition 0,NODE_LOCAL, 1939 bytes)
2017-07-01 16:44:34,060  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 5.0 (TID 11, localhost, partition 1,NODE_LOCAL, 1939 bytes)
2017-07-01 16:44:34,061  INFO [Executor task launch worker-5] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 5.0 (TID 10)
2017-07-01 16:44:34,063  INFO [Executor task launch worker-5] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:44:34,063  INFO [Executor task launch worker-5] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 0 ms
2017-07-01 16:44:34,064  INFO [Executor task launch worker-4] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 5.0 (TID 11)
2017-07-01 16:44:34,065  INFO [Executor task launch worker-4] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:44:34,065  INFO [Executor task launch worker-4] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 0 ms
2017-07-01 16:44:34,099  INFO [Executor task launch worker-5] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 5.0 (TID 10). 6094 bytes result sent to driver
2017-07-01 16:44:34,100  INFO [Executor task launch worker-4] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 5.0 (TID 11). 7074 bytes result sent to driver
2017-07-01 16:44:34,101  INFO [task-result-getter-2] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 5.0 (TID 10) in 41 ms on localhost (1/2)
2017-07-01 16:44:34,103  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 5.0 (TID 11) in 43 ms on localhost (2/2)
2017-07-01 16:44:34,103  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ResultStage 5 (collect at <console>:65) finished in 0,043 s
2017-07-01 16:44:34,103  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-07-01 16:44:34,103  INFO [Remote-akka.actor.default-dispatcher-18] (org.apache.spark.scheduler.DAGScheduler) - Job 2 finished: collect at <console>:65, took 0,411553 s
2017-07-01 16:44:35,278  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_8_piece0 on localhost:39618 in memory (size: 1515.0 B, free: 116.4 MB)
2017-07-01 16:44:35,290  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 8
2017-07-01 16:44:35,292  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_7_piece0 on localhost:39618 in memory (size: 7.2 KB, free: 116.4 MB)
2017-07-01 16:44:35,301  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 7
2017-07-01 16:44:35,304  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned shuffle 2
2017-07-01 16:45:40,423  INFO [Remote-akka.actor.default-dispatcher-17] (org.apache.spark.storage.MemoryStore) - Block broadcast_9 stored as values in memory (estimated size 187.6 KB, free 393.8 KB)
2017-07-01 16:45:40,448  INFO [Remote-akka.actor.default-dispatcher-17] (org.apache.spark.storage.MemoryStore) - Block broadcast_9_piece0 stored as bytes in memory (estimated size 19.5 KB, free 413.3 KB)
2017-07-01 16:45:40,451  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_9_piece0 in memory on localhost:39618 (size: 19.5 KB, free: 116.4 MB)
2017-07-01 16:45:40,455  INFO [Remote-akka.actor.default-dispatcher-17] (org.apache.spark.SparkContext) - Created broadcast 9 from textFile at <console>:54
2017-07-01 16:45:46,544  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.hadoop.mapred.FileInputFormat) - Total input paths to process : 1
2017-07-01 16:45:46,568  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.SparkContext) - Starting job: collect at <console>:65
2017-07-01 16:45:46,578  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Registering RDD 21 (map at <console>:64)
2017-07-01 16:45:46,580  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Got job 3 (collect at <console>:65) with 2 output partitions
2017-07-01 16:45:46,580  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Final stage: ResultStage 7 (collect at <console>:65)
2017-07-01 16:45:46,580  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Parents of final stage: List(ShuffleMapStage 6)
2017-07-01 16:45:46,580  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Missing parents: List(ShuffleMapStage 6)
2017-07-01 16:45:46,581  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ShuffleMapStage 6 (MapPartitionsRDD[21] at map at <console>:64), which has no missing parents
2017-07-01 16:45:46,582  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_10 stored as values in memory (estimated size 22.1 KB, free 435.5 KB)
2017-07-01 16:45:46,586  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.2 KB, free 442.7 KB)
2017-07-01 16:45:46,590  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_10_piece0 in memory on localhost:39618 (size: 7.2 KB, free: 116.4 MB)
2017-07-01 16:45:46,593  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 10 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:45:46,595  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[21] at map at <console>:64)
2017-07-01 16:45:46,595  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 6.0 with 2 tasks
2017-07-01 16:45:46,596  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 6.0 (TID 12, localhost, partition 0,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:45:46,601  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 6.0 (TID 13, localhost, partition 1,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:45:46,605  INFO [Executor task launch worker-6] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 6.0 (TID 12)
2017-07-01 16:45:46,609  INFO [Executor task launch worker-7] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 6.0 (TID 13)
2017-07-01 16:45:46,682  INFO [Executor task launch worker-6] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:0+5200
2017-07-01 16:45:46,686  INFO [Executor task launch worker-7] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:5200+5201
2017-07-01 16:45:46,789  INFO [Executor task launch worker-6] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 6.0 (TID 12). 2274 bytes result sent to driver
2017-07-01 16:45:46,794  INFO [task-result-getter-1] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 6.0 (TID 12) in 198 ms on localhost (1/2)
2017-07-01 16:45:46,799  INFO [Executor task launch worker-7] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 6.0 (TID 13). 2274 bytes result sent to driver
2017-07-01 16:45:46,801  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 6.0 (TID 13) in 205 ms on localhost (2/2)
2017-07-01 16:45:46,801  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2017-07-01 16:45:46,802  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ShuffleMapStage 6 (map at <console>:64) finished in 0,200 s
2017-07-01 16:45:46,802  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - looking for newly runnable stages
2017-07-01 16:45:46,802  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - running: Set()
2017-07-01 16:45:46,802  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - waiting: Set(ResultStage 7)
2017-07-01 16:45:46,802  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - failed: Set()
2017-07-01 16:45:46,802  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ResultStage 7 (ShuffledRDD[22] at reduceByKey at <console>:65), which has no missing parents
2017-07-01 16:45:46,803  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_11 stored as values in memory (estimated size 2.5 KB, free 445.2 KB)
2017-07-01 16:45:46,807  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_11_piece0 stored as bytes in memory (estimated size 1514.0 B, free 446.7 KB)
2017-07-01 16:45:46,809  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_11_piece0 in memory on localhost:39618 (size: 1514.0 B, free: 116.4 MB)
2017-07-01 16:45:46,809  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 11 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:45:46,811  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ResultStage 7 (ShuffledRDD[22] at reduceByKey at <console>:65)
2017-07-01 16:45:46,813  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 7.0 with 2 tasks
2017-07-01 16:45:46,815  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 7.0 (TID 14, localhost, partition 0,NODE_LOCAL, 1939 bytes)
2017-07-01 16:45:46,815  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 7.0 (TID 15, localhost, partition 1,NODE_LOCAL, 1939 bytes)
2017-07-01 16:45:46,817  INFO [Executor task launch worker-6] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 7.0 (TID 15)
2017-07-01 16:45:46,819  INFO [Executor task launch worker-7] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 7.0 (TID 14)
2017-07-01 16:45:46,819  INFO [Executor task launch worker-6] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:45:46,820  INFO [Executor task launch worker-6] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 1 ms
2017-07-01 16:45:46,822  INFO [Executor task launch worker-7] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:45:46,822  INFO [Executor task launch worker-7] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 0 ms
2017-07-01 16:45:46,831  INFO [Executor task launch worker-7] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 7.0 (TID 14). 6094 bytes result sent to driver
2017-07-01 16:45:46,833  INFO [task-result-getter-2] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 7.0 (TID 14) in 19 ms on localhost (1/2)
2017-07-01 16:45:46,838  INFO [Executor task launch worker-6] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 7.0 (TID 15). 7074 bytes result sent to driver
2017-07-01 16:45:46,840  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 7.0 (TID 15) in 25 ms on localhost (2/2)
2017-07-01 16:45:46,840  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2017-07-01 16:45:46,841  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ResultStage 7 (collect at <console>:65) finished in 0,027 s
2017-07-01 16:45:46,841  INFO [Remote-akka.actor.default-dispatcher-2] (org.apache.spark.scheduler.DAGScheduler) - Job 3 finished: collect at <console>:65, took 0,264039 s
2017-07-01 16:45:47,706  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_11_piece0 on localhost:39618 in memory (size: 1514.0 B, free: 116.4 MB)
2017-07-01 16:45:47,726  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 11
2017-07-01 16:45:47,729  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_10_piece0 on localhost:39618 in memory (size: 7.2 KB, free: 116.4 MB)
2017-07-01 16:45:47,737  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 10
2017-07-01 16:45:47,739  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned shuffle 3
2017-07-01 16:48:16,241  INFO [Remote-akka.actor.default-dispatcher-4] (org.apache.spark.storage.MemoryStore) - Block broadcast_12 stored as values in memory (estimated size 187.6 KB, free 600.9 KB)
2017-07-01 16:48:16,283  INFO [Remote-akka.actor.default-dispatcher-4] (org.apache.spark.storage.MemoryStore) - Block broadcast_12_piece0 stored as bytes in memory (estimated size 19.5 KB, free 620.5 KB)
2017-07-01 16:48:16,284  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_12_piece0 in memory on localhost:39618 (size: 19.5 KB, free: 116.4 MB)
2017-07-01 16:48:16,288  INFO [Remote-akka.actor.default-dispatcher-4] (org.apache.spark.SparkContext) - Created broadcast 12 from textFile at <console>:54
2017-07-01 16:48:21,492  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.hadoop.mapred.FileInputFormat) - Total input paths to process : 1
2017-07-01 16:48:21,513  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.SparkContext) - Starting job: collect at <console>:65
2017-07-01 16:48:21,519  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Registering RDD 27 (map at <console>:64)
2017-07-01 16:48:21,520  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Got job 4 (collect at <console>:65) with 2 output partitions
2017-07-01 16:48:21,520  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Final stage: ResultStage 9 (collect at <console>:65)
2017-07-01 16:48:21,520  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Parents of final stage: List(ShuffleMapStage 8)
2017-07-01 16:48:21,520  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Missing parents: List(ShuffleMapStage 8)
2017-07-01 16:48:21,520  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ShuffleMapStage 8 (MapPartitionsRDD[27] at map at <console>:64), which has no missing parents
2017-07-01 16:48:21,522  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_13 stored as values in memory (estimated size 22.2 KB, free 642.6 KB)
2017-07-01 16:48:21,529  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.2 KB, free 649.8 KB)
2017-07-01 16:48:21,530  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_13_piece0 in memory on localhost:39618 (size: 7.2 KB, free: 116.4 MB)
2017-07-01 16:48:21,531  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 13 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:48:21,531  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[27] at map at <console>:64)
2017-07-01 16:48:21,531  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 8.0 with 2 tasks
2017-07-01 16:48:21,532  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 8.0 (TID 16, localhost, partition 0,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:48:21,532  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 8.0 (TID 17, localhost, partition 1,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:48:21,537  INFO [Executor task launch worker-8] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 8.0 (TID 16)
2017-07-01 16:48:21,548  INFO [Executor task launch worker-9] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 8.0 (TID 17)
2017-07-01 16:48:21,614  INFO [Executor task launch worker-9] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:5200+5201
2017-07-01 16:48:21,615  INFO [Executor task launch worker-8] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:0+5200
2017-07-01 16:48:21,698  INFO [Executor task launch worker-8] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 8.0 (TID 16). 2274 bytes result sent to driver
2017-07-01 16:48:21,702  INFO [task-result-getter-1] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 8.0 (TID 16) in 170 ms on localhost (1/2)
2017-07-01 16:48:21,736  INFO [Executor task launch worker-9] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 8.0 (TID 17). 2274 bytes result sent to driver
2017-07-01 16:48:21,737  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 8.0 (TID 17) in 205 ms on localhost (2/2)
2017-07-01 16:48:21,737  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2017-07-01 16:48:21,737  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ShuffleMapStage 8 (map at <console>:64) finished in 0,204 s
2017-07-01 16:48:21,737  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - looking for newly runnable stages
2017-07-01 16:48:21,737  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - running: Set()
2017-07-01 16:48:21,737  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - waiting: Set(ResultStage 9)
2017-07-01 16:48:21,737  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - failed: Set()
2017-07-01 16:48:21,738  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ResultStage 9 (ShuffledRDD[28] at reduceByKey at <console>:65), which has no missing parents
2017-07-01 16:48:21,739  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_14 stored as values in memory (estimated size 2.5 KB, free 652.4 KB)
2017-07-01 16:48:21,750  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_14_piece0 stored as bytes in memory (estimated size 1516.0 B, free 653.9 KB)
2017-07-01 16:48:21,751  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_14_piece0 in memory on localhost:39618 (size: 1516.0 B, free: 116.4 MB)
2017-07-01 16:48:21,752  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 14 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:48:21,752  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ResultStage 9 (ShuffledRDD[28] at reduceByKey at <console>:65)
2017-07-01 16:48:21,752  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 9.0 with 2 tasks
2017-07-01 16:48:21,754  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 9.0 (TID 18, localhost, partition 0,NODE_LOCAL, 1939 bytes)
2017-07-01 16:48:21,754  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 9.0 (TID 19, localhost, partition 1,NODE_LOCAL, 1939 bytes)
2017-07-01 16:48:21,754  INFO [Executor task launch worker-8] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 9.0 (TID 19)
2017-07-01 16:48:21,756  INFO [Executor task launch worker-8] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:48:21,756  INFO [Executor task launch worker-8] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 0 ms
2017-07-01 16:48:21,757  INFO [Executor task launch worker-9] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 9.0 (TID 18)
2017-07-01 16:48:21,760  INFO [Executor task launch worker-9] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:48:21,763  INFO [Executor task launch worker-9] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 3 ms
2017-07-01 16:48:21,772  INFO [Executor task launch worker-8] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 9.0 (TID 19). 7074 bytes result sent to driver
2017-07-01 16:48:21,775  INFO [task-result-getter-2] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 9.0 (TID 19) in 21 ms on localhost (1/2)
2017-07-01 16:48:21,783  INFO [Executor task launch worker-9] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 9.0 (TID 18). 6094 bytes result sent to driver
2017-07-01 16:48:21,785  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 9.0 (TID 18) in 32 ms on localhost (2/2)
2017-07-01 16:48:21,785  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2017-07-01 16:48:21,785  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ResultStage 9 (collect at <console>:65) finished in 0,032 s
2017-07-01 16:48:21,786  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.scheduler.DAGScheduler) - Job 4 finished: collect at <console>:65, took 0,271554 s
2017-07-01 16:48:22,558  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_14_piece0 on localhost:39618 in memory (size: 1516.0 B, free: 116.4 MB)
2017-07-01 16:48:22,586  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 14
2017-07-01 16:48:22,587  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_13_piece0 on localhost:39618 in memory (size: 7.2 KB, free: 116.4 MB)
2017-07-01 16:48:22,589  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 13
2017-07-01 16:48:22,590  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned shuffle 4
2017-07-01 16:50:02,618  INFO [Remote-akka.actor.default-dispatcher-17] (org.apache.spark.storage.MemoryStore) - Block broadcast_15 stored as values in memory (estimated size 320.0 B, free 620.8 KB)
2017-07-01 16:50:02,625  INFO [Remote-akka.actor.default-dispatcher-17] (org.apache.spark.storage.MemoryStore) - Block broadcast_15_piece0 stored as bytes in memory (estimated size 206.0 B, free 621.0 KB)
2017-07-01 16:50:02,625  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_15_piece0 in memory on localhost:39618 (size: 206.0 B, free: 116.4 MB)
2017-07-01 16:50:02,628  INFO [Remote-akka.actor.default-dispatcher-17] (org.apache.spark.SparkContext) - Created broadcast 15 from broadcast at <console>:54
2017-07-01 16:50:07,696  INFO [Remote-akka.actor.default-dispatcher-4] (org.apache.spark.SparkContext) - Starting job: collect at <console>:65
2017-07-01 16:50:07,704  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Registering RDD 31 (map at <console>:64)
2017-07-01 16:50:07,704  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Got job 5 (collect at <console>:65) with 2 output partitions
2017-07-01 16:50:07,704  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Final stage: ResultStage 11 (collect at <console>:65)
2017-07-01 16:50:07,704  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Parents of final stage: List(ShuffleMapStage 10)
2017-07-01 16:50:07,704  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Missing parents: List(ShuffleMapStage 10)
2017-07-01 16:50:07,705  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ShuffleMapStage 10 (MapPartitionsRDD[31] at map at <console>:64), which has no missing parents
2017-07-01 16:50:07,707  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_16 stored as values in memory (estimated size 22.3 KB, free 643.2 KB)
2017-07-01 16:50:07,711  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_16_piece0 stored as bytes in memory (estimated size 7.2 KB, free 650.5 KB)
2017-07-01 16:50:07,717  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_16_piece0 in memory on localhost:39618 (size: 7.2 KB, free: 116.4 MB)
2017-07-01 16:50:07,720  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 16 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:50:07,721  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[31] at map at <console>:64)
2017-07-01 16:50:07,721  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 10.0 with 2 tasks
2017-07-01 16:50:07,722  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 10.0 (TID 20, localhost, partition 0,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:50:07,722  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 10.0 (TID 21, localhost, partition 1,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:50:07,732  INFO [Executor task launch worker-11] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 10.0 (TID 21)
2017-07-01 16:50:07,734  INFO [Executor task launch worker-10] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 10.0 (TID 20)
2017-07-01 16:50:07,793  INFO [Executor task launch worker-10] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:0+5200
2017-07-01 16:50:07,795  INFO [Executor task launch worker-11] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:5200+5201
2017-07-01 16:50:07,930  INFO [Executor task launch worker-10] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 10.0 (TID 20). 2274 bytes result sent to driver
2017-07-01 16:50:07,932  INFO [task-result-getter-1] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 10.0 (TID 20) in 211 ms on localhost (1/2)
2017-07-01 16:50:07,935  INFO [Executor task launch worker-11] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 10.0 (TID 21). 2274 bytes result sent to driver
2017-07-01 16:50:07,939  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 10.0 (TID 21) in 216 ms on localhost (2/2)
2017-07-01 16:50:07,939  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2017-07-01 16:50:07,939  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ShuffleMapStage 10 (map at <console>:64) finished in 0,218 s
2017-07-01 16:50:07,939  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - looking for newly runnable stages
2017-07-01 16:50:07,939  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - running: Set()
2017-07-01 16:50:07,939  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - waiting: Set(ResultStage 11)
2017-07-01 16:50:07,939  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - failed: Set()
2017-07-01 16:50:07,939  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ResultStage 11 (ShuffledRDD[32] at reduceByKey at <console>:65), which has no missing parents
2017-07-01 16:50:07,944  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_17 stored as values in memory (estimated size 2.5 KB, free 653.0 KB)
2017-07-01 16:50:07,949  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_17_piece0 stored as bytes in memory (estimated size 1517.0 B, free 654.5 KB)
2017-07-01 16:50:07,950  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_17_piece0 in memory on localhost:39618 (size: 1517.0 B, free: 116.4 MB)
2017-07-01 16:50:07,952  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 17 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:50:07,952  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[32] at reduceByKey at <console>:65)
2017-07-01 16:50:07,952  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 11.0 with 2 tasks
2017-07-01 16:50:07,953  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 11.0 (TID 22, localhost, partition 0,NODE_LOCAL, 1939 bytes)
2017-07-01 16:50:07,956  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 11.0 (TID 23, localhost, partition 1,NODE_LOCAL, 1939 bytes)
2017-07-01 16:50:07,956  INFO [Executor task launch worker-11] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 11.0 (TID 22)
2017-07-01 16:50:07,958  INFO [Executor task launch worker-10] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 11.0 (TID 23)
2017-07-01 16:50:07,960  INFO [Executor task launch worker-10] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:50:07,960  INFO [Executor task launch worker-10] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 0 ms
2017-07-01 16:50:07,960  INFO [Executor task launch worker-11] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:50:07,961  INFO [Executor task launch worker-11] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 1 ms
2017-07-01 16:50:07,968  INFO [Executor task launch worker-11] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 11.0 (TID 22). 4464 bytes result sent to driver
2017-07-01 16:50:07,970  INFO [task-result-getter-2] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 11.0 (TID 22) in 17 ms on localhost (1/2)
2017-07-01 16:50:07,979  INFO [Executor task launch worker-10] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 11.0 (TID 23). 5225 bytes result sent to driver
2017-07-01 16:50:07,981  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 11.0 (TID 23) in 28 ms on localhost (2/2)
2017-07-01 16:50:07,981  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2017-07-01 16:50:07,981  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ResultStage 11 (collect at <console>:65) finished in 0,025 s
2017-07-01 16:50:07,982  INFO [Remote-akka.actor.default-dispatcher-4] (org.apache.spark.scheduler.DAGScheduler) - Job 5 finished: collect at <console>:65, took 0,280554 s
2017-07-01 16:50:08,908  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_17_piece0 on localhost:39618 in memory (size: 1517.0 B, free: 116.4 MB)
2017-07-01 16:50:08,911  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 16
2017-07-01 16:50:08,914  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_16_piece0 on localhost:39618 in memory (size: 7.2 KB, free: 116.4 MB)
2017-07-01 16:50:08,915  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 15
2017-07-01 16:50:08,916  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned shuffle 5
2017-07-01 16:51:11,031  INFO [Remote-akka.actor.default-dispatcher-5] (org.apache.spark.storage.MemoryStore) - Block broadcast_18 stored as values in memory (estimated size 187.6 KB, free 808.6 KB)
2017-07-01 16:51:11,070  INFO [Remote-akka.actor.default-dispatcher-5] (org.apache.spark.storage.MemoryStore) - Block broadcast_18_piece0 stored as bytes in memory (estimated size 19.5 KB, free 828.1 KB)
2017-07-01 16:51:11,072  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_18_piece0 in memory on localhost:39618 (size: 19.5 KB, free: 116.3 MB)
2017-07-01 16:51:11,074  INFO [Remote-akka.actor.default-dispatcher-5] (org.apache.spark.SparkContext) - Created broadcast 18 from textFile at <console>:54
2017-07-01 16:51:13,212  INFO [Remote-akka.actor.default-dispatcher-16] (org.apache.spark.storage.MemoryStore) - Block broadcast_19 stored as values in memory (estimated size 384.0 B, free 828.5 KB)
2017-07-01 16:51:13,218  INFO [Remote-akka.actor.default-dispatcher-16] (org.apache.spark.storage.MemoryStore) - Block broadcast_19_piece0 stored as bytes in memory (estimated size 214.0 B, free 828.7 KB)
2017-07-01 16:51:13,222  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_19_piece0 in memory on localhost:39618 (size: 214.0 B, free: 116.3 MB)
2017-07-01 16:51:13,225  INFO [Remote-akka.actor.default-dispatcher-16] (org.apache.spark.SparkContext) - Created broadcast 19 from broadcast at <console>:54
2017-07-01 16:51:16,726  INFO [Remote-akka.actor.default-dispatcher-5] (org.apache.hadoop.mapred.FileInputFormat) - Total input paths to process : 1
2017-07-01 16:51:16,747  INFO [Remote-akka.actor.default-dispatcher-5] (org.apache.spark.SparkContext) - Starting job: collect at <console>:65
2017-07-01 16:51:16,751  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Registering RDD 37 (map at <console>:64)
2017-07-01 16:51:16,751  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Got job 6 (collect at <console>:65) with 2 output partitions
2017-07-01 16:51:16,751  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Final stage: ResultStage 13 (collect at <console>:65)
2017-07-01 16:51:16,751  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Parents of final stage: List(ShuffleMapStage 12)
2017-07-01 16:51:16,751  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Missing parents: List(ShuffleMapStage 12)
2017-07-01 16:51:16,752  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ShuffleMapStage 12 (MapPartitionsRDD[37] at map at <console>:64), which has no missing parents
2017-07-01 16:51:16,754  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_20 stored as values in memory (estimated size 22.4 KB, free 851.0 KB)
2017-07-01 16:51:16,758  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_20_piece0 stored as bytes in memory (estimated size 7.2 KB, free 858.2 KB)
2017-07-01 16:51:16,760  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_20_piece0 in memory on localhost:39618 (size: 7.2 KB, free: 116.3 MB)
2017-07-01 16:51:16,763  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 20 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:51:16,763  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[37] at map at <console>:64)
2017-07-01 16:51:16,763  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 12.0 with 2 tasks
2017-07-01 16:51:16,764  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 12.0 (TID 24, localhost, partition 0,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:51:16,765  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 12.0 (TID 25, localhost, partition 1,PROCESS_LOCAL, 2183 bytes)
2017-07-01 16:51:16,776  INFO [Executor task launch worker-12] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 12.0 (TID 24)
2017-07-01 16:51:16,789  INFO [Executor task launch worker-13] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 12.0 (TID 25)
2017-07-01 16:51:16,864  INFO [Executor task launch worker-13] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:5200+5201
2017-07-01 16:51:16,864  INFO [Executor task launch worker-12] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:0+5200
2017-07-01 16:51:16,926  INFO [Executor task launch worker-13] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 12.0 (TID 25). 2274 bytes result sent to driver
2017-07-01 16:51:16,928  INFO [task-result-getter-1] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 12.0 (TID 25) in 164 ms on localhost (1/2)
2017-07-01 16:51:16,953  INFO [Executor task launch worker-12] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 12.0 (TID 24). 2274 bytes result sent to driver
2017-07-01 16:51:16,955  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 12.0 (TID 24) in 191 ms on localhost (2/2)
2017-07-01 16:51:16,955  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2017-07-01 16:51:16,955  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ShuffleMapStage 12 (map at <console>:64) finished in 0,191 s
2017-07-01 16:51:16,955  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - looking for newly runnable stages
2017-07-01 16:51:16,955  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - running: Set()
2017-07-01 16:51:16,955  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - waiting: Set(ResultStage 13)
2017-07-01 16:51:16,955  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - failed: Set()
2017-07-01 16:51:16,956  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ResultStage 13 (ShuffledRDD[38] at reduceByKey at <console>:65), which has no missing parents
2017-07-01 16:51:16,957  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_21 stored as values in memory (estimated size 2.5 KB, free 860.8 KB)
2017-07-01 16:51:16,961  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_21_piece0 stored as bytes in memory (estimated size 1516.0 B, free 862.2 KB)
2017-07-01 16:51:16,962  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_21_piece0 in memory on localhost:39618 (size: 1516.0 B, free: 116.3 MB)
2017-07-01 16:51:16,964  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 21 from broadcast at DAGScheduler.scala:1006
2017-07-01 16:51:16,966  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ResultStage 13 (ShuffledRDD[38] at reduceByKey at <console>:65)
2017-07-01 16:51:16,967  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 13.0 with 2 tasks
2017-07-01 16:51:16,968  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 13.0 (TID 26, localhost, partition 0,NODE_LOCAL, 1939 bytes)
2017-07-01 16:51:16,968  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 13.0 (TID 27, localhost, partition 1,NODE_LOCAL, 1939 bytes)
2017-07-01 16:51:16,969  INFO [Executor task launch worker-12] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 13.0 (TID 26)
2017-07-01 16:51:16,969  INFO [Executor task launch worker-13] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 13.0 (TID 27)
2017-07-01 16:51:16,975  INFO [Executor task launch worker-13] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:51:16,975  INFO [Executor task launch worker-13] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 0 ms
2017-07-01 16:51:16,978  INFO [Executor task launch worker-12] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 16:51:16,979  INFO [Executor task launch worker-12] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 0 ms
2017-07-01 16:51:16,997  INFO [Executor task launch worker-12] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 13.0 (TID 26). 4328 bytes result sent to driver
2017-07-01 16:51:16,999  INFO [Executor task launch worker-13] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 13.0 (TID 27). 4656 bytes result sent to driver
2017-07-01 16:51:16,999  INFO [task-result-getter-2] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 13.0 (TID 26) in 31 ms on localhost (1/2)
2017-07-01 16:51:17,000  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 13.0 (TID 27) in 32 ms on localhost (2/2)
2017-07-01 16:51:17,000  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2017-07-01 16:51:17,001  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ResultStage 13 (collect at <console>:65) finished in 0,030 s
2017-07-01 16:51:17,002  INFO [Remote-akka.actor.default-dispatcher-5] (org.apache.spark.scheduler.DAGScheduler) - Job 6 finished: collect at <console>:65, took 0,251745 s
2017-07-01 16:51:17,478  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_20_piece0 on localhost:39618 in memory (size: 7.2 KB, free: 116.3 MB)
2017-07-01 16:51:17,497  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 18
2017-07-01 16:51:17,498  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned shuffle 6
2017-07-01 16:51:17,501  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_21_piece0 on localhost:39618 in memory (size: 1516.0 B, free: 116.3 MB)
2017-07-01 16:51:17,513  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 19
2017-07-01 17:04:37,663  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.storage.MemoryStore) - Block broadcast_22 stored as values in memory (estimated size 187.6 KB, free 1016.3 KB)
2017-07-01 17:04:37,787  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.storage.MemoryStore) - Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.5 KB, free 1035.8 KB)
2017-07-01 17:04:37,789  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_22_piece0 in memory on localhost:39618 (size: 19.5 KB, free: 116.3 MB)
2017-07-01 17:04:37,795  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.SparkContext) - Created broadcast 22 from textFile at <console>:54
2017-07-01 17:04:39,812  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.storage.MemoryStore) - Block broadcast_23 stored as values in memory (estimated size 416.0 B, free 1036.2 KB)
2017-07-01 17:04:41,867  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.storage.MemoryStore) - Block broadcast_23_piece0 stored as bytes in memory (estimated size 218.0 B, free 1036.4 KB)
2017-07-01 17:04:41,867  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_23_piece0 in memory on localhost:39618 (size: 218.0 B, free: 116.3 MB)
2017-07-01 17:04:41,868  INFO [Remote-akka.actor.default-dispatcher-3] (org.apache.spark.SparkContext) - Created broadcast 23 from broadcast at <console>:54
2017-07-01 17:04:45,135  INFO [Remote-akka.actor.default-dispatcher-17] (org.apache.hadoop.mapred.FileInputFormat) - Total input paths to process : 1
2017-07-01 17:04:45,162  INFO [Remote-akka.actor.default-dispatcher-17] (org.apache.spark.SparkContext) - Starting job: collect at <console>:65
2017-07-01 17:04:45,167  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Registering RDD 43 (map at <console>:64)
2017-07-01 17:04:45,167  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Got job 7 (collect at <console>:65) with 2 output partitions
2017-07-01 17:04:45,168  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Final stage: ResultStage 15 (collect at <console>:65)
2017-07-01 17:04:45,168  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Parents of final stage: List(ShuffleMapStage 14)
2017-07-01 17:04:45,168  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Missing parents: List(ShuffleMapStage 14)
2017-07-01 17:04:45,168  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ShuffleMapStage 14 (MapPartitionsRDD[43] at map at <console>:64), which has no missing parents
2017-07-01 17:04:45,182  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_24 stored as values in memory (estimated size 22.4 KB, free 1058.7 KB)
2017-07-01 17:04:45,187  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_24_piece0 stored as bytes in memory (estimated size 7.2 KB, free 1066.0 KB)
2017-07-01 17:04:45,205  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_24_piece0 in memory on localhost:39618 (size: 7.2 KB, free: 116.3 MB)
2017-07-01 17:04:45,208  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 24 from broadcast at DAGScheduler.scala:1006
2017-07-01 17:04:45,208  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[43] at map at <console>:64)
2017-07-01 17:04:45,209  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 14.0 with 2 tasks
2017-07-01 17:04:45,218  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 14.0 (TID 28, localhost, partition 0,PROCESS_LOCAL, 2183 bytes)
2017-07-01 17:04:45,219  INFO [dispatcher-event-loop-1] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 14.0 (TID 29, localhost, partition 1,PROCESS_LOCAL, 2183 bytes)
2017-07-01 17:04:45,220  INFO [Executor task launch worker-14] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 14.0 (TID 28)
2017-07-01 17:04:45,228  INFO [Executor task launch worker-15] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 14.0 (TID 29)
2017-07-01 17:04:45,307  INFO [Executor task launch worker-14] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:0+5200
2017-07-01 17:04:45,307  INFO [Executor task launch worker-15] (org.apache.spark.rdd.HadoopRDD) - Input split: file:/home/turmasabado/spark-notebook/README.md:5200+5201
2017-07-01 17:04:45,416  INFO [Executor task launch worker-14] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 14.0 (TID 28). 2274 bytes result sent to driver
2017-07-01 17:04:45,416  INFO [Executor task launch worker-15] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 14.0 (TID 29). 2274 bytes result sent to driver
2017-07-01 17:04:45,421  INFO [task-result-getter-1] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 14.0 (TID 29) in 201 ms on localhost (1/2)
2017-07-01 17:04:45,423  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 14.0 (TID 28) in 214 ms on localhost (2/2)
2017-07-01 17:04:45,423  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ShuffleMapStage 14 (map at <console>:64) finished in 0,214 s
2017-07-01 17:04:45,423  INFO [task-result-getter-0] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2017-07-01 17:04:45,423  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - looking for newly runnable stages
2017-07-01 17:04:45,423  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - running: Set()
2017-07-01 17:04:45,423  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - waiting: Set(ResultStage 15)
2017-07-01 17:04:45,423  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - failed: Set()
2017-07-01 17:04:45,424  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting ResultStage 15 (ShuffledRDD[44] at reduceByKey at <console>:65), which has no missing parents
2017-07-01 17:04:45,424  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_25 stored as values in memory (estimated size 2.5 KB, free 1068.5 KB)
2017-07-01 17:04:45,427  INFO [dag-scheduler-event-loop] (org.apache.spark.storage.MemoryStore) - Block broadcast_25_piece0 stored as bytes in memory (estimated size 1516.0 B, free 1070.0 KB)
2017-07-01 17:04:45,428  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Added broadcast_25_piece0 in memory on localhost:39618 (size: 1516.0 B, free: 116.3 MB)
2017-07-01 17:04:45,429  INFO [dag-scheduler-event-loop] (org.apache.spark.SparkContext) - Created broadcast 25 from broadcast at DAGScheduler.scala:1006
2017-07-01 17:04:45,429  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - Submitting 2 missing tasks from ResultStage 15 (ShuffledRDD[44] at reduceByKey at <console>:65)
2017-07-01 17:04:45,429  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.TaskSchedulerImpl) - Adding task set 15.0 with 2 tasks
2017-07-01 17:04:45,432  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 0.0 in stage 15.0 (TID 30, localhost, partition 0,NODE_LOCAL, 1939 bytes)
2017-07-01 17:04:45,432  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.TaskSetManager) - Starting task 1.0 in stage 15.0 (TID 31, localhost, partition 1,NODE_LOCAL, 1939 bytes)
2017-07-01 17:04:45,432  INFO [Executor task launch worker-14] (org.apache.spark.executor.Executor) - Running task 0.0 in stage 15.0 (TID 30)
2017-07-01 17:04:45,433  INFO [Executor task launch worker-15] (org.apache.spark.executor.Executor) - Running task 1.0 in stage 15.0 (TID 31)
2017-07-01 17:04:45,434  INFO [Executor task launch worker-15] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 17:04:45,435  INFO [Executor task launch worker-15] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 1 ms
2017-07-01 17:04:45,438  INFO [Executor task launch worker-14] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Getting 2 non-empty blocks out of 2 blocks
2017-07-01 17:04:45,438  INFO [Executor task launch worker-14] (org.apache.spark.storage.ShuffleBlockFetcherIterator) - Started 0 remote fetches in 0 ms
2017-07-01 17:04:45,452  INFO [Executor task launch worker-14] (org.apache.spark.executor.Executor) - Finished task 0.0 in stage 15.0 (TID 30). 4328 bytes result sent to driver
2017-07-01 17:04:45,453  INFO [Executor task launch worker-15] (org.apache.spark.executor.Executor) - Finished task 1.0 in stage 15.0 (TID 31). 4656 bytes result sent to driver
2017-07-01 17:04:45,454  INFO [task-result-getter-2] (org.apache.spark.scheduler.TaskSetManager) - Finished task 0.0 in stage 15.0 (TID 30) in 24 ms on localhost (1/2)
2017-07-01 17:04:45,456  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSetManager) - Finished task 1.0 in stage 15.0 (TID 31) in 24 ms on localhost (2/2)
2017-07-01 17:04:45,456  INFO [task-result-getter-3] (org.apache.spark.scheduler.TaskSchedulerImpl) - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2017-07-01 17:04:45,456  INFO [dag-scheduler-event-loop] (org.apache.spark.scheduler.DAGScheduler) - ResultStage 15 (collect at <console>:65) finished in 0,009 s
2017-07-01 17:04:45,457  INFO [Remote-akka.actor.default-dispatcher-17] (org.apache.spark.scheduler.DAGScheduler) - Job 7 finished: collect at <console>:65, took 0,294304 s
2017-07-01 17:04:46,223  INFO [dispatcher-event-loop-1] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_25_piece0 on localhost:39618 in memory (size: 1516.0 B, free: 116.3 MB)
2017-07-01 17:04:46,234  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 22
2017-07-01 17:04:46,236  INFO [dispatcher-event-loop-0] (org.apache.spark.storage.BlockManagerInfo) - Removed broadcast_24_piece0 on localhost:39618 in memory (size: 7.2 KB, free: 116.3 MB)
2017-07-01 17:04:46,243  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned accumulator 21
2017-07-01 17:04:46,244  INFO [Spark Context Cleaner] (org.apache.spark.ContextCleaner) - Cleaned shuffle 7
2017-07-01 17:38:24,472  INFO [Thread-1] (org.apache.spark.SparkContext) - Invoking stop() from shutdown hook
2017-07-01 17:38:24,530  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2017-07-01 17:38:24,531  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2017-07-01 17:38:24,533  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/api,null}
2017-07-01 17:38:24,533  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/,null}
2017-07-01 17:38:24,535  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/static,null}
2017-07-01 17:38:24,544  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2017-07-01 17:38:24,547  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2017-07-01 17:38:24,549  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2017-07-01 17:38:24,552  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/executors,null}
2017-07-01 17:38:24,553  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2017-07-01 17:38:24,561  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/environment,null}
2017-07-01 17:38:24,565  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2017-07-01 17:38:24,566  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2017-07-01 17:38:24,585  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2017-07-01 17:38:24,598  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/storage,null}
2017-07-01 17:38:24,600  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2017-07-01 17:38:24,601  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2017-07-01 17:38:24,602  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2017-07-01 17:38:24,604  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2017-07-01 17:38:24,605  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2017-07-01 17:38:24,606  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/stages,null}
2017-07-01 17:38:24,607  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2017-07-01 17:38:24,607  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2017-07-01 17:38:24,608  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2017-07-01 17:38:24,609  INFO [Thread-1] (org.spark-project.jetty.server.handler.ContextHandler) - stopped o.s.j.s.ServletContextHandler{/jobs,null}
2017-07-01 17:38:24,663  INFO [Thread-1] (org.apache.spark.ui.SparkUI) - Stopped Spark web UI at http://192.168.1.114:4040
2017-07-01 17:38:24,725  INFO [dispatcher-event-loop-0] (org.apache.spark.MapOutputTrackerMasterEndpoint) - MapOutputTrackerMasterEndpoint stopped!
2017-07-01 17:38:24,797  INFO [Thread-1] (org.apache.spark.storage.MemoryStore) - MemoryStore cleared
2017-07-01 17:38:24,799  INFO [Thread-1] (org.apache.spark.storage.BlockManager) - BlockManager stopped
2017-07-01 17:38:24,801  INFO [Thread-1] (org.apache.spark.storage.BlockManagerMaster) - BlockManagerMaster stopped
2017-07-01 17:38:24,806  INFO [dispatcher-event-loop-0] (org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint) - OutputCommitCoordinator stopped!
2017-07-01 17:38:24,816  INFO [Thread-1] (org.apache.spark.SparkContext) - Successfully stopped SparkContext
2017-07-01 17:38:24,817  INFO [Thread-1] (org.apache.spark.util.ShutdownHookManager) - Shutdown hook called
2017-07-01 17:38:24,820  INFO [Thread-1] (org.apache.spark.util.ShutdownHookManager) - Deleting directory /tmp/spark-cab7ad51-4cbd-4ed6-8dbf-d239120227f9/httpd-5663bb8e-4d02-460b-9a20-08f78fceb64a
2017-07-01 17:38:24,821  INFO [Thread-1] (org.apache.spark.util.ShutdownHookManager) - Deleting directory /tmp/spark-notebook-repl-d3d6d989-6eb4-4add-806e-7cb96d13dee4
2017-07-01 17:38:24,828  INFO [sparkDriverActorSystem-akka.actor.default-dispatcher-19] (akka.remote.RemoteActorRefProvider$RemotingTerminator) - Shutting down remote daemon.
2017-07-01 17:38:24,839  INFO [sparkDriverActorSystem-akka.actor.default-dispatcher-3] (akka.remote.RemoteActorRefProvider$RemotingTerminator) - Remote daemon shut down; proceeding with flushing remote transports.
2017-07-01 17:38:24,920  INFO [sparkDriverActorSystem-akka.actor.default-dispatcher-20] (akka.remote.RemoteActorRefProvider$RemotingTerminator) - Remoting shut down.
2017-07-01 17:38:25,048  WARN [main] (notebook.kernel.pfork.BetterFork$) - Parent process stopped; exiting.
2017-07-01 17:38:25,077  WARN [Remote-akka.actor.default-dispatcher-3] (akka.remote.ReliableDeliverySupervisor) - Association with remote system [akka.tcp://NotebookServer@127.0.0.1:59795] has failed, address is now gated for [5000] ms. Reason: [Disassociated] 
2017-07-01 17:38:25,089  INFO [Remote-akka.actor.default-dispatcher-3] (akka.remote.RemoteActorRefProvider$RemoteDeadLetterActorRef) - Message [org.apache.log4j.spi.LoggingEvent] from Actor[akka://Remote/user/remote-logger#387925155] to Actor[akka://Remote/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
2017-07-01 17:38:25,089  INFO [Remote-akka.actor.default-dispatcher-18] (akka.remote.RemoteActorRefProvider$RemoteDeadLetterActorRef) - Message [org.apache.log4j.spi.LoggingEvent] from Actor[akka://Remote/user/remote-logger#387925155] to Actor[akka://Remote/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
2017-07-01 17:38:25,090  INFO [Remote-akka.actor.default-dispatcher-18] (akka.remote.RemoteActorRefProvider$RemoteDeadLetterActorRef) - Message [org.apache.log4j.spi.LoggingEvent] from Actor[akka://Remote/user/remote-logger#387925155] to Actor[akka://Remote/deadLetters] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
2017-07-01 17:38:25,090  INFO [Remote-akka.actor.default-dispatcher-3] (akka.remote.RemoteActorRefProvider$RemoteDeadLetterActorRef) - Message [org.apache.log4j.spi.LoggingEvent] from Actor[akka://Remote/user/remote-logger#387925155] to Actor[akka://Remote/deadLetters] was not delivered. [4] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
2017-07-01 17:38:25,090  INFO [Remote-akka.actor.default-dispatcher-18] (akka.remote.RemoteActorRefProvider$RemoteDeadLetterActorRef) - Message [org.apache.log4j.spi.LoggingEvent] from Actor[akka://Remote/user/remote-logger#387925155] to Actor[akka://Remote/deadLetters] was not delivered. [5] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
2017-07-01 17:38:25,091  INFO [Remote-akka.actor.default-dispatcher-3] (akka.remote.RemoteActorRefProvider$RemoteDeadLetterActorRef) - Message [org.apache.log4j.spi.LoggingEvent] from Actor[akka://Remote/user/remote-logger#387925155] to Actor[akka://Remote/deadLetters] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
2017-07-01 17:38:25,091  INFO [Remote-akka.actor.default-dispatcher-18] (akka.remote.RemoteActorRefProvider$RemoteDeadLetterActorRef) - Message [org.apache.log4j.spi.LoggingEvent] from Actor[akka://Remote/user/remote-logger#387925155] to Actor[akka://Remote/deadLetters] was not delivered. [7] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
2017-07-01 17:38:25,091  INFO [Remote-akka.actor.default-dispatcher-3] (akka.remote.RemoteActorRefProvider$RemoteDeadLetterActorRef) - Message [org.apache.log4j.spi.LoggingEvent] from Actor[akka://Remote/user/remote-logger#387925155] to Actor[akka://Remote/deadLetters] was not delivered. [8] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
2017-07-01 17:38:25,091  INFO [Remote-akka.actor.default-dispatcher-18] (akka.remote.RemoteActorRefProvider$RemoteDeadLetterActorRef) - Message [org.apache.log4j.spi.LoggingEvent] from Actor[akka://Remote/user/remote-logger#387925155] to Actor[akka://Remote/deadLetters] was not delivered. [9] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
2017-07-01 17:38:25,092  INFO [Remote-akka.actor.default-dispatcher-3] (akka.remote.RemoteActorRefProvider$RemoteDeadLetterActorRef) - Message [org.apache.log4j.spi.LoggingEvent] from Actor[akka://Remote/user/remote-logger#387925155] to Actor[akka://Remote/deadLetters] was not delivered. [10] dead letters encountered, no more dead letters will be logged. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
2017-07-01 17:38:25,328  INFO [Thread-1] (org.apache.spark.util.ShutdownHookManager) - Deleting directory /tmp/spark-cab7ad51-4cbd-4ed6-8dbf-d239120227f9
