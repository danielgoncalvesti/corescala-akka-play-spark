{
  "metadata" : {
    "name" : "spark-lab02",
    "user_save_timestamp" : "1969-12-31T21:00:00.000Z",
    "auto_save_timestamp" : "1969-12-31T21:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : [ "-Xmx512m" ],
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "20FCFA5AEF2D4BD380CC97088ABCF686"
    },
    "cell_type" : "markdown",
    "source" : "# Laboratorio 2 - Contagem de palavras com Spark Notebook utilizando acumuladores e broadcast\n\nNeste laboratório iremos incrementar o contador de palavras criado no laboratório anterior para ignorar palavras com simbolos como *, (, ), etc.\n\nIremos criar uma variável de broadcast que deve conter os simbolos proibidos. Todas as palavras que contenham algum dos simbolos devem ser excluidas da contagem de palavras.\n\nAlém disso, vamos utilizar um acumulador que deve conter o número de palavras excluidas da contagem.\n\nOs simbolos proibidos devem ser determinados a partir de diversas execuções do programa seguindo o algoritmo:\n<ul>\n  <li>executar a primeira vez o programa sem excluir nenhuma palavra</li>\n  <li>identificar palavras com simbolos como (,),*, etc</li>\n  <li>adicionar os simbolos encontrados na lista de broadcast de simbolos proibidos</li>\n  <li>executar novamente o programa até eliminar todas as palavras com simbolos</li>\n</ul>\n\nOBS: Os seguintes simbolos são permitidos nas palavras: '-' e '’'\n\nSeguindo o exemplo de contagem utilizado no laboratório anterior e o modelo de uso de acumuladores e variáveis de broadcast apresentados na apostila, implemente o código pedido a seguir:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B8DB24727F474997A1A8AFB4BBC39D67"
    },
    "cell_type" : "code",
    "source" : "//Primeiro vamos carregar o conteudo do arquivo README.MD.\nval input = sc.textFile(\"README.md\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "input: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[40] at textFile at <console>:54\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 67
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "EAC48D5AF7034EF781970C4C52485AD9"
    },
    "cell_type" : "code",
    "source" : "//Criar um acumulador para contagem do número de palavras excluidas\nval ignored = sc.accumulator(0)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "ignored: org.apache.spark.Accumulator[Int] = 0\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 68
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0DB25B394E5344D783C82DCFE2D3EB52"
    },
    "cell_type" : "code",
    "source" : "//E uma variável de broadcast contendo a lista de simbolos que devem ser excluidos\nval forbiddenSymbols = sc.broadcast(List('/', '(', ':', '=', '#', '$', '!', '[', ' ', '-', '|', '<'))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "forbiddenSymbols: org.apache.spark.broadcast.Broadcast[List[Char]] = Broadcast(23)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 69
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4E69D499766249458A33B16A7A6751F2"
    },
    "cell_type" : "code",
    "source" : "//Quebrar o arquivo carregado em uma lista de palavras.\nval words = input.flatMap(line => line.split(\" \"))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[41] at flatMap at <console>:56\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 70
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "74A6B12B19E74424A9A24E9871C6853B"
    },
    "cell_type" : "code",
    "source" : "/* \nFiltrar as palavras para utilizar somente aquelas que não contém os simbolos proibidos.\nNão se esqueça de incrementar o contador quando uma palavra for excluida. \n*/\nval filtered = words.filter( word => \n if(word.toCharArray.intersect(forbiddenSymbols.value).isEmpty){\n    true\n }\n  else {\n    ignored += 1 \n    false\n  })",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "filtered: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[42] at filter at <console>:65\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 71
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E7A71566E0F54336837FF2281F0FE5F8"
    },
    "cell_type" : "code",
    "source" : "//Aplicar o algoritmo map/reduce de contagem de palavras conforme visto na apostila.\nval counts = filtered.map(word => (word,1))\n         .reduceByKey( (x,y) => x+y).collect()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "counts: Array[(String, Int)] = Array((someone,1), (Partner,1), (queries,,2), (offer,1), (health,1), (garment,1), (Open,2), (entirely,1), (*Originally,1), (C'mon,1), (some,2), (only,1), (complex,1), (we,2), (This,1), (industry.,2), (basic,1), (documentation,1), (Amino,1), (Checkout,1), (read,1), (Korea,1), (network,1), (banking,1), (Meanwhile,,1), (Massive,1), (Download,1), (out,2), (Data,2), (Metail,1), (Agile,1), (tool,2), (file,1), (analysis,2), (performs,1), (&,2), (rewritten.,1), (problems,1), (Dataset,1), (distribution.,1), (London.,1), (mobile,1), (provides,1), (addition,,1), (advanced,2), (can,2), (when,1), (Italian,1), (Apache,1), (CloudPhysics,1), (our,3), (>,2), (interactive,1), (identity.,1), (Swisscom,2), (science,1), (Want,1), (Start,1), (services,1), (Databricks,1), (Scala..."
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 72
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "026F2B007966466C89E47FB078A2CC13"
    },
    "cell_type" : "code",
    "source" : "//Imprimir cada uma das palavras encontradas e sua respectiva contagem.\ncounts.foreach{ case (word,count) => println(f\"$word%20s \\t $count\")}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "             someone \t 1\n             Partner \t 1\n            queries, \t 2\n               offer \t 1\n              health \t 1\n             garment \t 1\n                Open \t 2\n            entirely \t 1\n         *Originally \t 1\n               C'mon \t 1\n                some \t 2\n                only \t 1\n             complex \t 1\n                  we \t 2\n                This \t 1\n           industry. \t 2\n               basic \t 1\n       documentation \t 1\n               Amino \t 1\n            Checkout \t 1\n                read \t 1\n               Korea \t 1\n             network \t 1\n             banking \t 1\n          Meanwhile, \t 1\n             Massive \t 1\n            Download \t 1\n                 out \t 2\n                Data \t 2\n              Metail \t 1\n               Agile \t 1\n                tool \t 2\n                file \t 1\n            analysis \t 2\n            performs \t 1\n                   & \t 2\n          rewritten. \t 1\n            problems \t 1\n             Dataset \t 1\n       distribution. \t 1\n             London. \t 1\n              mobile \t 1\n            provides \t 1\n           addition, \t 1\n            advanced \t 2\n                 can \t 2\n                when \t 1\n             Italian \t 1\n              Apache \t 1\n        CloudPhysics \t 1\n                 our \t 3\n                   > \t 2\n         interactive \t 1\n           identity. \t 1\n            Swisscom \t 2\n             science \t 1\n                Want \t 1\n               Start \t 1\n            services \t 1\n          Databricks \t 1\n               Scala \t 2\n             leading \t 2\n             Cluster \t 1\n            variable \t 1\n            Analysis \t 1\n       computational \t 1\n            options, \t 1\n                firm \t 1\n                EMR, \t 1\n              Aliyun \t 3\n              create \t 1\n             systems \t 1\n        Testimonials \t 1\n              Vinted \t 1\n     visualisations, \t 1\n                  is \t 10\n        specializing \t 1\n         Description \t 1\n      configuration, \t 1\n              Change \t 1\n                even \t 1\n              simply \t 1\n                 one \t 2\n               start \t 1\n                tech \t 1\n                with \t 1\n                  Go \t 1\n                 mix \t 1\n                data \t 4\n            outdated \t 1\n            straight \t 1\n             sources \t 2\n              world. \t 1\n               using \t 1\n             SMARTER \t 1\n          consulting \t 1\n                life \t 1\n             runtime \t 1\n                 new \t 1\n                root \t 1\n                  We \t 1\n             window. \t 1\n               gives \t 1\n                from \t 4\n             details \t 1\n            choices. \t 1\n             options \t 1\n             women’s \t 1\n                best \t 1\n              allows \t 2\n                  us \t 1\n            learning \t 2\n                body \t 2\n            training \t 1\n             Science \t 1\n        experiments, \t 1\n               Learn \t 1\n             discuss \t 1\n                deep \t 1\n               this. \t 1\n                 SQL \t 1\n     `sparkContext`. \t 1\n              Markup \t 1\n              useful \t 1\n                EMBL \t 1\n            Adopters \t 1\n                 get \t 2\n                     \t 4120\n                 Mad \t 1\n           ecosystem \t 1\n                   * \t 7\n           prototype \t 1\n             cluster \t 1\n           Notebook? \t 1\n          programme, \t 1\n             browser \t 1\n              Scala, \t 1\n              almost \t 1\n            geeks)** \t 1\n           Computing \t 1\n             BigData \t 1\n        cofiguration \t 1\n               young \t 1\n               we’re \t 1\n              steps. \t 1\n                  IT \t 1\n             British \t 1\n                 For \t 1\n               Spark \t 9\n           certified \t 1\n       collaborative \t 1\n                  it \t 1\n                 The \t 7\n             Mesons, \t 1\n            Typesafe \t 1\n               facts \t 1\n         researchers \t 1\n       headquartered \t 1\n             focused \t 1\n           extensive \t 1\n         dashboards. \t 1\n             Degrees \t 2\n               help, \t 1\n                   A \t 1\n                  kt \t 2\n             through \t 1\n         convenient. \t 1\n            scalable \t 1\n            achieved \t 1\n             manner. \t 1\n          lifestyle. \t 1\n             company \t 3\n           Certified \t 1\n           somewhere \t 1\n              needed \t 1\n                 for \t 2\n             biology \t 1\n              freely \t 1\n                  To \t 1\n            Scalable \t 1\n               these \t 1\n                 Lab \t 1\n                 sbt \t 1\n                your \t 2\n          supporting \t 1\n                 the \t 21\n                 URL \t 1\n            research \t 1\n            Skymind, \t 1\n          refactored \t 1\n             explain \t 1\n                  BI \t 1\n             latest, \t 1\n               shape \t 1\n           different \t 1\n                  47 \t 2\n        Switzerland. \t 1\n                 fit \t 1\n                 ECS \t 2\n                most \t 1\n                  Do \t 1\n             answers \t 1\n          pipelines. \t 1\n           financial \t 1\n           community \t 1\n                  At \t 1\n                demo \t 1\n          integrator \t 1\n              Spark. \t 1\n              editor \t 1\n            analysts \t 1\n                 FOR \t 1\n       intelligence. \t 1\n           Institute \t 1\n                  by \t 1\n              clean, \t 1\n                care \t 1\n                user \t 1\n                  It \t 1\n                like \t 1\n               Quick \t 1\n                  an \t 2\n             machine \t 2\n                 but \t 1\n         environment \t 1\n           flexible, \t 1\n             Execute \t 1\n                NexR \t 2\n          developers \t 1\n            Barclays \t 2\n            tackling \t 1\n               more. \t 1\n              forked \t 1\n               about \t 2\n                 run \t 2\n          everyone’s \t 1\n            provider \t 1\n          performing \t 1\n                  on \t 4\n              users) \t 1\n              social \t 1\n             Extract \t 1\n              global \t 1\n            features \t 1\n               code, \t 1\n             service \t 1\n              YARN), \t 1\n                  in \t 9\n              offers \t 1\n              online \t 1\n                  In \t 1\n              builds \t 1\n               **Run \t 1\n          unstable). \t 1\n                 try \t 1\n                meet \t 1\n             amazing \t 1\n      Bioinformatics \t 1\n            INSIGHTS \t 1\n               2007. \t 1\n                 way \t 2\n            expanded \t 1\n                 use \t 1\n                Name \t 1\n               prose \t 1\n             running \t 1\n                find \t 1\n             command \t 1\n            Notebook \t 2\n                box, \t 1\n           directory \t 1\n       multinational \t 1\n              Online \t 1\n          customers. \t 1\n                that \t 1\n                   a \t 6\n                  or \t 3\n                 you \t 1\n          JavaScript \t 1\n             perform \t 1\n             empower \t 1\n                  to \t 16\n           available \t 2\n                code \t 1\n             Skymind \t 2\n                more \t 1\n            academia \t 1\n                 see \t 1\n                  of \t 6\n         marketplace \t 1\n                you. \t 1\n               FAQs. \t 1\n          management \t 1\n            European \t 1\n               where \t 1\n               power \t 1\n          deployment \t 1\n             things, \t 1\n                 and \t 20\n            accessed \t 1\n               being \t 1\n             combine \t 1\n                Logo \t 1\n              Fellas \t 1\n              Vingle \t 2\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 73
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DA991841CE7941BCBB3BF66CA146EB95"
    },
    "cell_type" : "code",
    "source" : "//Imprimir o número de palavras ignoradas\nprintln(s\"Total de palavras ignoradas ${ignored.value}\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Total de palavras ignoradas 153\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 74
    } ]
  }, {
    "metadata" : {
      "id" : "6B61DC6D6F554554831BB17E2B4E8E42"
    },
    "cell_type" : "markdown",
    "source" : "O resultado esperado é ignorar 209 palavras!"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C5CFF5F812E249D1876B4834F404C8CD"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 75
    } ]
  } ],
  "nbformat" : 4
}